"""
LPM (Longest Prefix Match) Scheduling Simulator
SGLangì˜ LPM ìŠ¤ì¼€ì¤„ë§ì„ ëª¨ì‚¬í•˜ëŠ” HTTP ì„œë²„

ê¸°ë³¸ì ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ í¬í•¨í•œ HTTP ìš”ì²­ì„ ìˆ˜ì‹ í•˜ë©´ Ollamaì— í•´ë‹¹ í”„ë¡¬í”„íŠ¸ë¥¼ ì „ì†¡í•˜ê³  ì‘ë‹µì„ ë°˜í™˜
ë‹¨, ìš”ì²­ì„ ë³´ë‚´ê¸° ì „ LPM ìŠ¤ì¼€ì¥´ë§ì„ ë³¸ ì½”ë“œì—ì„œ ìˆ˜í–‰
KV-CacheëŠ” ë‹¨ìˆœíˆ ë”•ì…”ë„ˆë¦¬ë¡œ êµ¬í˜„
ì¦‰, SGLangì˜ LPMì˜ í•µì‹¬ì¸ ê°€ì¥ ê¸´ ë§¤ì¹˜ë¥¼ ê°€ì§„ ìš”ì²­ì„ ìš°ì„  ì²˜ë¦¬ ë¶€ë¶„ë§Œ êµ¬í˜„
"""
import json
import logging
import time
from http.server import BaseHTTPRequestHandler, HTTPServer, ThreadingHTTPServer
from typing import Dict, Tuple
from queue import PriorityQueue, Queue
from dataclasses import dataclass, field
import urllib.request
import threading

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s'
)
logger = logging.getLogger(__name__)

# Configuration
OLLAMA_URL = "http://localhost:11434/api/generate"
OLLAMA_MODEL = "llama3.2:1b"
LPM_SERVER_PORT = 9000
OLLAMA_TIMEOUT = 720  # Ollama ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
CLIENT_RESPONSE_TIMEOUT = 725  # í´ë¼ì´ì–¸íŠ¸ ì‘ë‹µ ëŒ€ê¸° íƒ€ì„ì•„ì›ƒ (ì´ˆ)


@dataclass(order=True)
class Request:
    """ìš”ì²­ ì •ë³´ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    priority: int = field(compare=True)  # ìŒìˆ˜ê°’ (ê¸´ ë§¤ì¹˜ì¼ìˆ˜ë¡ ìš°ì„ ìˆœìœ„ ë†’ìŒ)
    timestamp: float = field(compare=True)
    prompt: str = field(compare=False)
    request_id: str = field(compare=False)
    response_queue: Queue = field(compare=False)
    cancelled: threading.Event = field(compare=False, default_factory=threading.Event)


class KVCache:
    """KV ìºì‹œë¥¼ ì‹œë®¬ë ˆì´ì…˜í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.cache: Dict[str, float] = {}  # {prompt: cached_timestamp}
        self.lock = threading.Lock()
    
    def add(self, prompt: str):
        """ìºì‹œì— í”„ë¡¬í”„íŠ¸ ì¶”ê°€"""
        with self.lock:
            self.cache[prompt] = time.time()
            logger.info(f"[CACHE] Added: '{prompt[:50]}...' (Total cached: {len(self.cache)})")
    
    def find_longest_prefix_match(self, prompt: str) -> Tuple[str, int]:
        """
        ì£¼ì–´ì§„ í”„ë¡¬í”„íŠ¸ì™€ ê°€ì¥ ê¸´ ì ‘ë‘ì‚¬ ë§¤ì¹­ì„ ê°€ì§„ ìºì‹œ ì—”íŠ¸ë¦¬ ì°¾ê¸°
        ë‹¨ë°©í–¥ ë§¤ì¹­: ìºì‹œëœ í”„ë¡¬í”„íŠ¸ê°€ ìƒˆ ìš”ì²­ì˜ ì ‘ë‘ì‚¬ì¸ì§€ í™•ì¸
        Returns: (matched_prefix, match_length)
        """
        with self.lock:
            longest_match = ""
            max_length = 0
            
            logger.debug(f"[CACHE] Finding LPM for prompt: '{prompt[:50]}...' (Cache size: {len(self.cache)})")
            
            for cached_prompt in self.cache.keys():
                # ë‹¨ë°©í–¥ ë§¤ì¹­: cached_promptê°€ promptë¡œ ì‹œì‘í•˜ëŠ”ì§€ í™•ì¸
                # (ìºì‹œëœ ê¸´ í”„ë¡¬í”„íŠ¸ê°€ ìƒˆ ìš”ì²­ì„ prefixë¡œ ê°€ì§€ëŠ”ì§€)
                if cached_prompt.startswith(prompt):
                    if len(cached_prompt) > max_length:
                        max_length = len(cached_prompt)
                        longest_match = cached_prompt
            
            if max_length > 0:
                logger.info(f"[CACHE] LPM HIT: match_length={max_length}, matched='{longest_match[:50]}...'")
            else:
                logger.info(f"[CACHE] LPM MISS: No prefix match found")
            
            return longest_match, max_length
    
    def clear(self):
        """ìºì‹œ ì´ˆê¸°í™”"""
        with self.lock:
            cache_size = len(self.cache)
            self.cache.clear()
            logger.info(f"[CACHE] Cleared {cache_size} entries")
    
    def dump_state(self):
        """í˜„ì¬ ìºì‹œ ìƒíƒœ ë¤í”„ (ë””ë²„ê¹…ìš©)"""
        with self.lock:
            logger.info(f"[CACHE] === Cache State Dump (Total: {len(self.cache)}) ===")
            for i, (prompt, timestamp) in enumerate(self.cache.items(), 1):
                logger.info(f"[CACHE]   {i}. '{prompt[:60]}...' (cached at {timestamp:.3f})")
            logger.info(f"[CACHE] === End of Cache Dump ===")


class LPMScheduler:
    """LPM ìŠ¤ì¼€ì¤„ë§ì„ ìˆ˜í–‰í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, kv_cache: KVCache, num_workers: int = 4):
        self.kv_cache = kv_cache
        self.request_queue = PriorityQueue()
        self.worker_threads = []
        self.num_workers = num_workers
        self.running = True
        self.active_requests = []  # í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ ìš”ì²­ë“¤ ì¶”ì 
        self.active_requests_lock = threading.Lock()
    
    def submit_request(self, prompt: str, request_id: str) -> tuple[Queue, Request]:
        """ìš”ì²­ì„ ìŠ¤ì¼€ì¤„ëŸ¬ì— ì œì¶œ"""
        # LPM: ê°€ì¥ ê¸´ prefix ë§¤ì¹˜ë¥¼ ì°¾ì•„ì„œ ìš°ì„ ìˆœìœ„ ê³„ì‚°
        matched_prefix, match_length = self.kv_cache.find_longest_prefix_match(prompt)
        
        # ìš°ì„ ìˆœìœ„: ë§¤ì¹˜ ê¸¸ì´ê°€ ê¸¸ìˆ˜ë¡ ë†’ì€ ìš°ì„ ìˆœìœ„ (ìŒìˆ˜ ì‚¬ìš©)
        priority = -match_length
        
        response_queue = Queue()
        request = Request(
            priority=priority,
            timestamp=time.time(),
            prompt=prompt,
            request_id=request_id,
            response_queue=response_queue
        )
        
        if match_length > 0:
            logger.info(f"[SCHEDULE] Request '{request_id}' - Cache HIT (match_length={match_length}, priority={priority})")
        else:
            logger.info(f"[SCHEDULE] Request '{request_id}' - Cache MISS (priority={priority})")
        
        logger.debug(f"[SCHEDULE] Queuing request '{request_id}' with prompt: '{prompt[:50]}...'")
        self.request_queue.put(request)
        
        return response_queue, request
    
    def start_worker(self):
        """ì›Œì»¤ ìŠ¤ë ˆë“œ ì‹œì‘ (ë©€í‹° ì›Œì»¤)"""
        for i in range(self.num_workers):
            worker_thread = threading.Thread(target=self._process_requests, args=(i,), daemon=True)
            worker_thread.start()
            self.worker_threads.append(worker_thread)
        logger.info(f"[SCHEDULER] Started {self.num_workers} worker threads")
    
    def _process_requests(self, worker_id: int):
        """ìš”ì²­ì„ ìš°ì„ ìˆœìœ„ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬ (ë©€í‹° ì›Œì»¤)"""
        logger.info(f"[WORKER-{worker_id}] Started")
        while self.running:
            try:
                # ìš°ì„ ìˆœìœ„ íì—ì„œ ìš”ì²­ ê°€ì ¸ì˜¤ê¸° (ìš°ì„ ìˆœìœ„ ë†’ì€ ê²ƒë¶€í„°)
                request = self.request_queue.get(timeout=1.0)
                
                # ì·¨ì†Œëœ ìš”ì²­ì€ ê±´ë„ˆëœ€
                if request.cancelled.is_set():
                    logger.info(f"[WORKER-{worker_id}] Request '{request.request_id}' cancelled, skipping")
                    request.response_queue.put("Error: Request cancelled")
                    self.request_queue.task_done()
                    continue
                
                # í™œì„± ìš”ì²­ ëª©ë¡ì— ì¶”ê°€
                with self.active_requests_lock:
                    self.active_requests.append(request)
                
                start_time = time.time()
                logger.info(f"[WORKER-{worker_id}] âš¡ START Processing '{request.request_id}' - Priority={request.priority}, Prompt: '{request.prompt[:50]}...'")
                
                # LPM ì²´í¬: ìºì‹œì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                matched_prefix, match_length = self.kv_cache.find_longest_prefix_match(request.prompt)
                is_cache_hit = match_length > 0
                
                if is_cache_hit:
                    # ìºì‹œ íˆíŠ¸: ì¦‰ì‹œ ì‘ë‹µ (Ollama í˜¸ì¶œ ì—†ìŒ)
                    logger.info(f"[WORKER-{worker_id}] ğŸš€ CACHE HIT! Responding immediately (match_length={match_length})")
                    if not request.cancelled.is_set():
                        # ë”ë¯¸ ì‘ë‹µ ì¦‰ì‹œ ë°˜í™˜ (ì‹¤ì œ SGLangì€ ìºì‹œëœ í† í° ì¬ì‚¬ìš©)
                        response_text = "[CACHED] Response based on cached KV"
                        request.response_queue.put(response_text)
                    else:
                        request.response_queue.put("Error: Request cancelled")
                else:
                    # ìºì‹œ ë¯¸ìŠ¤: Ollama ì²˜ë¦¬ í•„ìš”
                    logger.info(f"[WORKER-{worker_id}] â„ï¸  CACHE MISS - Processing with Ollama")
                    
                    # ìºì‹œì— ë¨¼ì € ì¶”ê°€ (ë‹¤ìŒ ìš”ì²­ì„ ìœ„í•´)
                    if not request.cancelled.is_set():
                        cache_add_time = time.time()
                        self.kv_cache.add(request.prompt)
                        logger.info(f"[WORKER-{worker_id}] âœ“ Cached '{request.request_id}' at {cache_add_time:.3f}")
                    
                    # Ollamaì— ìš”ì²­ ì „ì†¡ (ì·¨ì†Œ ì²´í¬ í¬í•¨)
                    if not request.cancelled.is_set():
                        ollama_start = time.time()
                        logger.debug(f"[WORKER-{worker_id}] Sending to Ollama: '{request.request_id}'")
                        response_text = self._send_to_ollama(request.prompt)
                        ollama_duration = time.time() - ollama_start
                        logger.debug(f"[WORKER-{worker_id}] Ollama responded in {ollama_duration:.3f}s")
                        
                        # ì‘ë‹µ ì „ì†¡ ì „ ë‹¤ì‹œ ì·¨ì†Œ í™•ì¸
                        if not request.cancelled.is_set():
                            # ì‘ë‹µì„ ìš”ì²­ìì—ê²Œ ì „ë‹¬
                            request.response_queue.put(response_text)
                        else:
                            request.response_queue.put("Error: Request cancelled")
                    else:
                        request.response_queue.put("Error: Request cancelled")
                
                # í™œì„± ìš”ì²­ ëª©ë¡ì—ì„œ ì œê±°
                with self.active_requests_lock:
                    if request in self.active_requests:
                        self.active_requests.remove(request)
                
                end_time = time.time()
                total_duration = end_time - start_time
                remaining_requests = self.request_queue.qsize()
                logger.info(f"[WORKER-{worker_id}] âœ“ DONE '{request.request_id}' in {total_duration:.3f}s (Queue: {remaining_requests} remaining)")
                
                self.request_queue.task_done()
                
            except:
                # íƒ€ì„ì•„ì›ƒì´ë‚˜ ë‹¤ë¥¸ ì˜ˆì™¸ëŠ” ë¬´ì‹œí•˜ê³  ê³„ì† ëŒ€ê¸°
                continue
    
    def flush_all(self):
        """ëª¨ë“  ìš”ì²­ì„ ì¤‘ë‹¨í•˜ê³  ìºì‹œ ì´ˆê¸°í™”"""
        logger.info("[SCHEDULER] Flushing all requests and cache...")
        
        # 1. í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ ëª¨ë“  ìš”ì²­ì— ì·¨ì†Œ í”Œë˜ê·¸ ì„¤ì •
        with self.active_requests_lock:
            for request in self.active_requests:
                request.cancelled.set()
            active_count = len(self.active_requests)
        
        # 2. íì— ìˆëŠ” ëª¨ë“  ìš”ì²­ì„ êº¼ë‚´ì„œ ì·¨ì†Œ ì²˜ë¦¬
        cancelled_queue_count = 0
        while not self.request_queue.empty():
            try:
                request = self.request_queue.get_nowait()
                request.cancelled.set()
                request.response_queue.put("Error: Request cancelled by flush")
                self.request_queue.task_done()
                cancelled_queue_count += 1
            except:
                break
        
        # 3. ìºì‹œ ì´ˆê¸°í™”
        self.kv_cache.clear()
        
        logger.info(f"[SCHEDULER] Flushed: {active_count} active requests, {cancelled_queue_count} queued requests")
    
    def _send_to_ollama(self, prompt: str) -> str:
        """Ollamaì— ì‹¤ì œ ìš”ì²­ ì „ì†¡"""
        payload = {
            "model": OLLAMA_MODEL,
            "prompt": prompt,
            "stream": False,
            "options": {
                "num_predict": 16
            }
        }
        
        try:
            req = urllib.request.Request(
                OLLAMA_URL,
                data=json.dumps(payload).encode('utf-8'),
                headers={'Content-Type': 'application/json'}
            )
            
            with urllib.request.urlopen(req, timeout=OLLAMA_TIMEOUT) as response:
                resp_body = response.read().decode('utf-8')
                resp_json = json.loads(resp_body)
                answer = resp_json.get("response", "")
                return answer
                
        except Exception as e:
            logger.warning(f"[OLLAMA] Error: {e}")
            return f"Error: {str(e)}"


# Global instances
kv_cache = KVCache()
scheduler = LPMScheduler(kv_cache)


class LPMRequestHandler(BaseHTTPRequestHandler):
    """LPM ì„œë²„ì˜ HTTP ìš”ì²­ í•¸ë“¤ëŸ¬"""
    
    def log_message(self, format, *args):
        """ë¡œê·¸ ë©”ì‹œì§€ë¥¼ loggerë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸"""
        logger.debug(f"HTTP: {format % args}")
    
    def do_POST(self):
        """POST ìš”ì²­ ì²˜ë¦¬"""
        if self.path == "/generate":
            request_obj = None
            try:
                content_length = int(self.headers.get('Content-Length', 0))
                post_data = self.rfile.read(content_length).decode('utf-8')
                
                if not post_data:
                    self.send_response(400)
                    self.end_headers()
                    self.wfile.write(b"No input provided")
                    return
                
                request_id = f"req_{int(time.time() * 1000000)}"
                
                # ìŠ¤ì¼€ì¤„ëŸ¬ì— ìš”ì²­ ì œì¶œ
                response_queue, request_obj = scheduler.submit_request(post_data, request_id)
                
                # ì‘ë‹µ ëŒ€ê¸° (í´ë¼ì´ì–¸íŠ¸ íƒ€ì„ì•„ì›ƒë³´ë‹¤ ì•½ê°„ ì§§ê²Œ)
                response_text = response_queue.get(timeout=CLIENT_RESPONSE_TIMEOUT)
                
                self.send_response(200)
                self.send_header("Content-type", "text/plain; charset=utf-8")
                self.end_headers()
                self.wfile.write(response_text.encode('utf-8'))
                
            except BrokenPipeError:
                logger.debug(f"[HTTP] Client disconnected (BrokenPipe)")
                if request_obj:
                    request_obj.cancelled.set()
                return
            except ConnectionResetError:
                logger.debug(f"[HTTP] Client disconnected (ConnectionReset)")
                if request_obj:
                    request_obj.cancelled.set()
                return
            except Exception as e:
                logger.error(f"[HTTP] Error processing request: {type(e).__name__}: {e}")
                if request_obj:
                    request_obj.cancelled.set()
                try:
                    self.send_response(500)
                    self.end_headers()
                except:
                    pass  # í´ë¼ì´ì–¸íŠ¸ê°€ ì´ë¯¸ ì—°ê²°ì„ ëŠì€ ê²½ìš°
                return
        
        elif self.path == "/flush_cache":
            scheduler.flush_all()
            self.send_response(200)
            self.send_header("Content-type", "text/plain")
            self.end_headers()
            self.wfile.write(b"All requests cancelled and cache flushed")
        
        else:
            self.send_response(404)
            self.end_headers()
    
    def do_GET(self):
        """GET ìš”ì²­ ì²˜ë¦¬"""
        if self.path == "/health":
            self.send_response(200)
            self.send_header("Content-type", "text/plain")
            self.end_headers()
            self.wfile.write(b"LPM Server is running")
        else:
            self.send_response(404)
            self.end_headers()


def run_server(port: int = LPM_SERVER_PORT):
    """LPM ì„œë²„ ì‹¤í–‰"""
    # ìŠ¤ì¼€ì¤„ëŸ¬ ì›Œì»¤ ì‹œì‘
    scheduler.start_worker()
    
    server_address = ('', port)
    # ThreadingHTTPServerë¡œ ë³€ê²½í•˜ì—¬ ë©€í‹°ìŠ¤ë ˆë“œ ìš”ì²­ ì²˜ë¦¬
    httpd = ThreadingHTTPServer(server_address, LPMRequestHandler)
    logger.info(f"LPM Server starting on port {port}...")
    logger.info(f"Ollama URL: {OLLAMA_URL}")
    logger.info(f"Ollama Model: {OLLAMA_MODEL}")
    logger.info(f"Number of worker threads: {scheduler.num_workers}")
    
    try:
        httpd.serve_forever()
    except KeyboardInterrupt:
        logger.info("Server stopped by user")
        scheduler.running = False
        httpd.shutdown()


if __name__ == "__main__":
    run_server()
